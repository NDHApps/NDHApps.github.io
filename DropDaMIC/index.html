<!DOCTYPE html>
<html>
    <head>
        <title>DropDa MIC</title>
        <meta name="viewport" content="width=device-width,initial-scale=1.0,user-scalable=no">
        <link rel="stylesheet" type="text/css" href="css/materialize.css">
        <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    </head>
    <body class="splash">
        <header>
            <nav class="top-nav">
                <div class="container">
                    <div class="nav-wrapper">
                        <a href="index.html" class="left brand-logo">
                            <div class="nav-item-wrapper">
                                <img class="nav-item" src="logo.png">
                                <div class="nav-item">
                                    DropDa MIC 
                                </div>
                            </div>
                        </a>
                        <a href="#" data-activates="mobile-demo" class="button-collapse"><i class="material-icons">menu</i></a>
                        <ul class="right hide-on-med-and-down">
                            <li><a href="#About">About</a></li>
                            <li><a href="#Method">Method</a></li>
                            <li><a href="#Results">Results</a></li>
                            <li><a href="GitHub">GitHub</a></li>
                        </ul>
                        <ul class="side-nav" id="mobile-demo">
                            <li><a href="index.html">Home</a></li>
                            <li><a href="#About">About</a></li>
                            <li><a href="#Method">Method</a></li>
                            <li><a href="#Results">Results</a></li>
                            <li><a href="GitHub">GitHub</a></li>
                        </ul>
                    </div>
                </div>
            </nav>
        </header>
        <div class="main container">
            <div class="left">
                <div class="section">
                    <h4 class="header deep-purple-text">DropDa MIC (Musical Instrument Classifier)</h4>
                    <h6>Final Project for EECS 352: Machine Perception of Music and Audio</h6>
                    <h6>Northwestern University</h6>
                    <h6>Professor Bryan Pardo</h6>
                    <div class="row">
                        <div class="col s12 m6 l4">
                            <h5>Hannah Arntson</h5>
                            <h6 class="truncate">HannahArntson2016@u.northwestern.edu</h6>
                        </div>
                        <div class="col s12 m6 l4">
                            <h5>Nicholas Hall</h5>
                            <h6 class="truncate">NicholasHall2016@u.northwestern.edu</h6>
                        </div>
                        <div class="col s12 m6 l4">
                            <h5>Jason Lustbader</h5>
                            <h6 class="truncate">JasonLustbader2016@u.northwestern.edu</h6>
                        </div>
                    </div>
                </div>
                <div class="divider"></div>
                <div class="section" id="About">
                    <h4 class="deep-purple-text">About</h4>
                    <h5>Motivation</h5>
                    <div class="flow-text">
                        We are all very musical and can easily tell the difference between different instruments and groupings of instruments. We think it would be interesting to teach a computer to make the same differentiations.
                    </div>
                    <h5>Description</h5>
                    <div class="flow-text">
                        Our team will be identifying the instruments used in various classical music recordings. We will create a system that can tell the difference between a solo piano, a solo horn, a string quartet, an entire orchestra, etc. We will take a .wav file as input. The user will specify the file either on the command line or as a website upload, depending of our final interface. The file will contain audio from a recording of a classical piece of music. The system will then compare the recording to the training samples and use K-nearest neighbors to identify the sample's instrumentation.
                        <br><br>
                    </div>
                </div>
                <div class="divider"></div>
                <div class="section" id="Method">
                    <h4 class="deep-purple-text">Method</h4>
                    <div class="flow-text">
                        We used musical instrument samples from the <a href="http://theremin.music.uiowa.edu/MIS.html">University of Iowa Musical Instrument Samples (MIS)</a> dataset. We picked five (5) very different sounding instruments to put into our database: flute, bass clarinet, french horn, violin, and double bass. We loaded several short 2-3 second .wav samples of each using Librosa and then used the middle-most second of each audio file to account for silence at the beginning and end of notes. From there, we divided each audio sample into 2048 sample frames. For each sample, we calculated discrete Fourier transform (DFT) magnitudes, mel-frequency cepstral coefficients (MFCCs), and energy changes from frame to frame and recorded them along with the label (the instrument) in a hash table so we wouldn't have to recalculate every time we called the classifier. Next, we created a K-nearest neighbor classifier using <a href="http://scikit-learn.org/stable/">scikit-learn</a>. When a new input file is given, we would (like the other files) load it in with Librosa, crop it, divide it into frames and calculate the features- for each frame, we would calculate the minimum euclidean distance from the input feature space to each sample frame in our hash table's feature space. After all the frames had been tested, the instrument with the largest number of frames that were labelled as nearest-neighbor were said to be the algorithm's "guess."
                        <br><br>
                    </div>
                </div>
                <div class="divider"></div>
                <div class="section" id="Results">
                    <h4 class="deep-purple-text">Results</h4>
                    <div class="flow-text">
                        When we compared two very different instruments--we started with a flute and a trumpet--our classifier was very accurate. However, when we used a full training set that contained 5 instruments (flute, bass clarinet, french horn, violin, and double bass), with 44-104 distinct samples of each instrument over a wide range of notes, we found some interesting results.
                        <br><br>  
                        Almost all of our recordings ended up being labeled as stringed instruments, either violin or double bass. A few were labeled as flutes, and not one recording was labeled as a bass clarinet or french horn, despite running 46 and 44 samples, respectively, of each instrument through the classifier. We are unsure of why the classifier ended up being heavily weighted in favor or stringed instruments. However, we did notice that each instrument's lowest pitched recordings tended to get classified as double bass, while the higher recordings were labeled as violin. This trend was noticeable across the actual violin and double bass recordings as well.
                        <br><br>  
                        To further improve our classifier, we would spend more time experimenting with feature weights.
                        <br><br>
                    </div>
                </div>
            </div>
        </div>
        <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
        <script type="text/javascript" src="js/index.js"></script>
        <script type="text/javascript" src="js/materialize.min.js"></script>
    </body>
</html>